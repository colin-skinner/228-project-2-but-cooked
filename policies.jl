using Printf
using Serialization
using Distributions
using StatsBase

function save_policy(filename::String, policy_func::Function, num_states::Int)
    open(filename, "w") do f
        for si in 1:num_states
            write(f, @sprintf("%d\n", policy_func(si)))
        end
    end
end

function save_action_value_function(filename::String, model)
    serialize(filename, model)
end

function load_action_value_function(filename::String)
    return deserialize(filename)
end

function get_lines(filename::String)
    lines = Vector{Vector{Int}}()
    open(filename, "r") do input
        header = readline(input) # Header is ignored

        # Parse lines
        for line in eachline(input)
            sample = parse.(Int, split(line, ','))
            push!(lines, sample)
        end
    end
    return lines
end

##################################################
#   Q Learning
##################################################

mutable struct QLearning
    ğ’® # state space (assumes 1:nstates)
    ğ’œ # action space (assumes 1:nactions)
    Î³ # discount
    Q # action value function
    Î± # learning rate
end

lookahead(model::QLearning, s, a) = model.Q[s,a]

function update!(model::QLearning, s, a, r, sâ€²)
    Î³, Q, Î± = model.Î³, model.Q, model.Î±
    Q[s,a] += Î±*(r + Î³*maximum(Q[sâ€²,:]) - Q[s,a])
    return model
end

##################################################
#   MaximumLikelihoodMDP
##################################################

mutable struct MaximumLikelihoodMDP
    ğ’® # state space (assumes 1:nstates)
    ğ’œ # action space (assumes 1:nactions)
    N # transition count N(s,a,sâ€²) - dictionary based
    Ï # reward sum Ï(s, a) - dictionary based
    Î³ # discount
    U # value function

end

function lookahead(model::MaximumLikelihoodMDP, s, a)
    ğ’®, U, Î³ = model.ğ’®, model.U, model.Î³
    key = (s, a)

    state_dict = get(model.N, key, Dict())
    if isempty(state_dict)
        return 0.0
    end
    
    n = sum(values(state_dict))
    r = model.Ï[key] / n
    
    return r + Î³ * sum((count/n) * U[sâ€²] for (sâ€², count) in state_dict)
end

function backup(model::MaximumLikelihoodMDP, U, s)
    vals = [lookahead(model, s, a) for a in model.ğ’œ]
    return isempty(vals) ? 0.0 : maximum(vals)
end

function update!(model::MaximumLikelihoodMDP, s, a, r, sâ€²)
    key = (s, a)

    state_dict = get(model.N, key, Dict{Int,Int}())
    state_dict[sâ€²] = get(state_dict, sâ€², 0) + 1

    model.N[key] = state_dict
    model.Ï[key] = get(model.Ï, key, 0.0) + r

    return model
end

##################################################
#   ValueFunctionPolicy
##################################################

struct ValueFunctionPolicy
    ğ’« # problem
    U # utility function
end

function greedy(ğ’«::MaximumLikelihoodMDP, U::Vector{Float64}, s::Int)
    u, a = findmax(a -> lookahead(ğ’«, s, a), ğ’«.ğ’œ)
    return (a=a, u=u)
end

(Ï€::ValueFunctionPolicy)(s) = greedy(Ï€.ğ’«, Ï€.U, s).a

##################################################
#   ValueIteration
##################################################

struct ValueIteration
    k_max # maximum number of iterations
end

function solve(M::ValueIteration, ğ’«::MaximumLikelihoodMDP)
    U = [0.0 for s in ğ’«.ğ’®]
    for k = 1:M.k_max
        U = [backup(ğ’«, U, s) for s in ğ’«.ğ’®]
    end
    # Update the model's value function
    ğ’«.U = U
    return ValueFunctionPolicy(ğ’«, U)
end

##################################################
#   Softmax
##################################################

mutable struct SoftmaxExploration
    Î» # precision parameter
    Î± # precision factor
end

function normalize(weights)
    total = sum(weights)
    return weights ./ total
end

function (Ï€::SoftmaxExploration)(model::MaximumLikelihoodMDP, s)
    # Q-values for actions
    Q_values = [lookahead(model, s, a) for a in model.ğ’œ]
    
    Q_shift = Q_values .- maximum(Q_values)
    weights = exp.(Ï€.Î» * Q_shift)
    Ï€.Î» *= Ï€.Î±
    
    # return s sampled action
    return rand(Categorical(normalize(weights)))
end