using Printf
using Serialization

function save_policy(filename::String, policy_func::Function, num_states::Int)
    open(filename, "w") do f
        for si in 1:num_states
            write(f, @sprintf("%d\n", policy_func(si)))
        end
    end
end

function save_action_value_function(filename::String, model)
    serialize(filename, model)
end

function load_action_value_function(filename::String)
    return deserialize(filename)
end

function get_lines(filename::String)

    lines = Vector{Vector{Int}}()
    open(filename, "r") do input
        header = readline(input) # Header is ignored

        # Parse lines
        for line in eachline(input)
            sample = parse.(Int, split(line, ','))
            push!(lines, sample)
        end
    end
    return lines
end

##################################################
#   Q Learning
##################################################

mutable struct QLearning
    ğ’® # state space (assumes 1:nstates)
    ğ’œ # action space (assumes 1:nactions)
    Î³ # discount
    Q # action value function
    Î± # learning rate
end

lookahead(model::QLearning, s, a) = model.Q[s,a]

function update!(model::QLearning, s, a, r, sâ€²)
    Î³, Q, Î± = model.Î³, model.Q, model.Î±
    Q[s,a] += Î±*(r + Î³*maximum(Q[sâ€²,:]) - Q[s,a])
    return model
end

##################################################
#   MaximumLikelihoodMDP
##################################################

mutable struct MaximumLikelihoodMDP
    ğ’® # state space (assumes 1:nstates)
    ğ’œ # action space (assumes 1:nactions)
    N # transition count N(s,a,sâ€²)
    Ï # reward sum Ï(s, a)
    Î³ # discount
    U # value function
    planner
end

function lookahead(model::MaximumLikelihoodMDP, U::Vector{Float64}, s::Int, a::Int)
    key = (s,a)
    nextdict = get(model.N, key, Dict())
    if isempty(nextdict)
        return 0.0
    end
    total = sum(values(nextdict))
    r = model.Ï[key] / total
    ev = sum((cnt/total) * U[sp] for (sp,cnt) in nextdict)
    return r + model.Î³ * ev
end

function backup(model::MaximumLikelihoodMDP, U::Vector{Float64}, s::Int)
    vals = [lookahead(model, U, s, a) for a in model.ğ’œ]
    return isempty(vals) ? 0.0 : maximum(vals)
end

function update!(model::MaximumLikelihoodMDP, s::Int, a::Int, r::Number, sâ€²::Int)
    key = (s,a)
    nextdict = get(model.N, key, Dict{Int,Float64}())
    nextdict[sâ€²] = get(nextdict, sâ€², 0.0) + 1.0
    model.N[key] = nextdict
    model.Ï[key] = get(model.Ï, key, 0.0) + float(r)
end

struct ValueFunctionPolicy
    ğ’« # problem
    U # utility function
end
function greedy(ğ’«::MaximumLikelihoodMDP, U::Vector{Float64}, s::Int)
    u, a = findmax(a -> lookahead(ğ’«, U, s, a), ğ’«.ğ’œ)
    return (a=a, u=u)
end

(Ï€::ValueFunctionPolicy)(s) = greedy(Ï€.ğ’«, Ï€.U, s).a

function softmax_probs(Q::Vector{<:Real}, Ï„::Float64)
    Q_shift = Q .- maximum(Q)         # prevent overflow
    exps = exp.(Q_shift ./ Ï„)
    return exps ./ sum(exps)
end

# Sample an index from a probability vector p (sums to 1)
function sample_index(p::Vector{<:Real})
    r = rand()          # random number in [0,1)
    cumsum_val = 0.0
    for (i, prob) in enumerate(p)
        cumsum_val += prob
        if r < cumsum_val
            return i     # return 1-based index
        end
    end
    return length(p)    # fallback in case of rounding error
end

# Softmax action selection
function softmax_action(Q::Vector{<:Real}, Ï„::Float64)
    p = softmax_probs(Q, Ï„)
    return sample_index(p)
end

# Wrapper for MaximumLikelihoodMDP: returns action given state
function softmax_policy(model::MaximumLikelihoodMDP, U::Vector{Float64}, s::Int, Ï„::Float64)
    # compute lookahead values for all actions
    Q = [lookahead(model, U, s, a) for a in model.ğ’œ]
    return softmax_action(Q, Ï„)
end

# Make a callable policy function
function make_softmax_policy(policy::ValueFunctionPolicy, Ï„::Float64)
    return s -> softmax_policy(policy.ğ’«, policy.U, s, Ï„)
end
struct ValueIteration
    k_max::Int
    tol::Float64
end

function ValueIteration(k_max::Int; tol=1e-6)
    return ValueIteration(k_max, tol)
end

function solve(M::ValueIteration, ğ’«::MaximumLikelihoodMDP)
    n = length(ğ’«.ğ’®)
    U = zeros(Float64, n)
    U_new = similar(U)

    for k in 1:M.k_max
        maxdiff = 0.0
        for s in ğ’«.ğ’®
            u_s = backup(ğ’«, U, s)
            U_new[s] = u_s
            maxdiff = max(maxdiff, abs(U[s]-u_s))
        end
        copy!(U, U_new)
        if maxdiff < M.tol
            break
        end
    end

    ğ’«.U = U
    return ValueFunctionPolicy(ğ’«, U)
end

make_policy_function(policy::ValueFunctionPolicy) = s -> greedy(policy.ğ’«, policy.U, s).a

# Îµ-greedy policy wrapper for MLE MDP
function make_epsilon_greedy_policy(policy::ValueFunctionPolicy, Îµ::Float64, default_action::Int=1)
    return s -> begin
        # handle unseen or out-of-bounds states
        if s < 1 || s > length(policy.ğ’«.ğ’®)
            return default_action
        end
        if rand() < Îµ
            # explore randomly
            return rand(policy.ğ’«.ğ’œ)
        else
            # greedy action
            return greedy(policy.ğ’«, policy.U, s).a
        end
    end
end

# Revised train_max_likelihood
function train_max_likelihood(
        name, csv_name, cache_name, save_name,
        rows, cols, rate, discount = 0.95,
        iters = 1000, k_max = 300, Îµ = 0.05)

    # Load or initialize MLE MDP
    if isfile(cache_name)
        max_likelihood = load_action_value_function(cache_name)
        println("Loaded cached MDP")
    else
        planner = ValueIteration(k_max)
        max_likelihood = MaximumLikelihoodMDP(
            1:rows, 1:cols, Dict(), Dict(), discount, zeros(rows), planner
        )
    end

    # Load dataset
    lines = get_lines(csv_name)

    println("Training $name")
    for i in 1:iters
        for line in lines
            update!(max_likelihood, line[1], line[2], line[3], line[4])
        end
        if i % 50 == 0
            println("Iteration $i / $iters")
        end
    end

    # Value iteration
    policy = solve(max_likelihood.planner, max_likelihood)
    println("Value iteration done")

    # Îµ-greedy policy function
    Îµ_policy = make_epsilon_greedy_policy(policy, Îµ, default_action=1)

    # Save policy and MDP
    save_policy(save_name, Îµ_policy, rows)
    save_action_value_function(cache_name, max_likelihood)

    println("Saved $name policy and MDP cache")
end


# 341 for MaximumLikelihoodMDP
# 318 for FullUpdate