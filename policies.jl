using Printf
using Serialization

# EpsilonGreedyExploration?
# PolicyIteratio



function save_policy(filename::String, policy_func::Function, num_states::Int)
    open(filename, "w") do f
        for si in 1:num_states
            write(f, @sprintf("%d\n", policy_func(si)))
        end
    end
end

function save_action_value_function(filename::String, model)
    serialize(filename, model)
end

function load_action_value_function(filename::String)
    return deserialize(filename)
end

##################################################
#   Q Learning
##################################################

mutable struct QLearning
    ğ’® # state space (assumes 1:nstates)
    ğ’œ # action space (assumes 1:nactions)
    Î³ # discount
    Q # action value function
    Î± # learning rate
end

lookahead(model::QLearning, s, a) = model.Q[s,a]

function update!(model::QLearning, s, a, r, sâ€²)
    Î³, Q, Î± = model.Î³, model.Q, model.Î±
    Q[s,a] += Î±*(r + Î³*maximum(Q[sâ€²,:]) - Q[s,a])
    return model
end

function get_lines(filename::String)

    lines = Vector{Vector{Int}}()
    open(filename, "r") do input
        header = readline(input) # Header is ignored

        # Parse lines
        for line in eachline(input)
            sample = parse.(Int, split(line, ','))
            push!(lines, sample)
        end
    end
    return lines
end

##################################################
#   MaximumLikelihoodMDP
##################################################

mutable struct MaximumLikelihoodMDP
    ğ’® # state space (assumes 1:nstates)
    ğ’œ # action space (assumes 1:nactions)
    N # transition count N(s,a,sâ€²)
    Ï # reward sum Ï(s, a)
    Î³ # discount
    U # value function
    planner
end

function lookahead(model::MaximumLikelihoodMDP, U::AbstractVector{<:Real}, s::Int, a::Int)
    key = (s,a)
    nextdict = get(model.N, key, nothing)
    if nextdict === nothing || isempty(nextdict)
        return 0.0
    end
    # Sum counts and compute expected reward
    n = 0.0
    for c in values(nextdict)
        n += c
    end
    if n == 0.0
        return 0.0
    end
    r = model.Ï[key] / n
    # expected value
    ev = 0.0
    for (sp, cnt) in nextdict
        ev += (cnt / n) * U[sp]
    end
    return r + model.Î³ * ev
end

function backup(model::MaximumLikelihoodMDP, U::AbstractVector{<:Real}, s::Int)
    best = -Inf
    for a in model.ğ’œ
        val = lookahead(model, U, s, a)
        if val > best
            best = val
        end
    end
    # if no actions or all -Inf (shouldn't happen) return 0.0
    return isfinite(best) ? best : 0.0
end

function update!(model::MaximumLikelihoodMDP, s::Int, a::Int, r::Number, sâ€²::Int)
    key = (s,a)
    # lazy-create per-(s,a) dict
    nextdict = get(model.N, key, nothing)
    if nextdict === nothing
        nextdict = Dict{Int,Float64}()
        model.N[key] = nextdict
    end
    nextdict[sâ€²] = get(nextdict, sâ€², 0.0) + 1.0
    model.Ï[key] = get(model.Ï, key, 0.0) + float(r)
    return model
end

struct ValueFunctionPolicy
    ğ’« # problem
    U # utility function
end
function greedy(ğ’«::MaximumLikelihoodMDP, U, s)
    u, a = findmax(a->lookahead(ğ’«, U, s, a), ğ’«.ğ’œ)
    return (a=a, u=u)
end

(Ï€::ValueFunctionPolicy)(s) = greedy(Ï€.ğ’«, Ï€.U, s).a


struct ValueIteration
    k_max::Int
    tol::Float64
end

function ValueIteration(k_max::Int; tol=1e-6)
    return ValueIteration(k_max, tol)
end

function solve(M::ValueIteration, ğ’«::MaximumLikelihoodMDP)
    n = length(ğ’«.ğ’®)
    U = zeros(Float64, n)         # current values (indexed 1..n)
    U_new = similar(U)

    for k in 1:M.k_max
        # compute in-place, avoid allocations inside loop
        maxdiff = 0.0
        for s in ğ’«.ğ’®
            u_s = backup(ğ’«, U, s)
            U_new[s] = u_s
            diff = abs(u_s - U[s])
            if diff > maxdiff
                maxdiff = diff
            end
        end
        # swap / copy
        copy!(U, U_new)
        # early stopping
        if maxdiff < M.tol
            # println("Value iteration converged at iter $k (maxdiff=$maxdiff)")
            break
        end
    end

    ğ’«.U = U  # store final values back in model
    return ValueFunctionPolicy(ğ’«, U)
end

function make_policy_function(policy::ValueFunctionPolicy)
    return s -> greedy(policy.ğ’«, policy.U, s).a
end


# 341 for MaximumLikelihoodMDP
# 318 for FullUpdate